<html>
<title>Wiki Page</title>

<head>
    

</head>

<body onhashchange="changeFunction()">
    <a href="#machinelearnig">Machine Learnig</a>
    <br>
    <a href="#application">Application</a>
    <br>
    <a href="#ethics">Ethics</a>
    <br>
    <a href="#approaches">Approaches</a>
    <br>
    <p id="print"></p>

    <p id=machinelearnig>
        Machine learning is a subset of artificial intelligence in the field of computer science that often uses statistical techniques
        to give computers the ability to "learn" (i.e., progressively improve performance on a specific task) with data,
        without being explicitly programmed.[1] The name machine learning was coined in 1959 by Arthur Samuel.[2] Evolved
        from the study of pattern recognition and computational learning theory in artificial intelligence,[3] machine learning
        explores the study and construction of algorithms that can learn from and make predictions on data[4] â€“ such algorithms
        overcome following strictly static program instructions by making data-driven predictions or decisions,[5]:2 through
        building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and
        programming explicit algorithms with good performance is difficult or infeasible; example applications include email
        filtering, detection of network intruders or malicious insiders working towards a data breach,[6] optical character
        recognition (OCR),[7] learning to rank, and computer vision. Machine learning is closely related to (and often overlaps
        with) computational statistics, which also focuses on prediction-making through the use of computers. It has strong
        ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning
        is sometimes conflated with data mining,[8] where the latter subfield focuses more on exploratory data analysis and
        is known as unsupervised learning.[5]:vii[9] Machine learning can also be unsupervised[10] and be used to learn and
        establish baseline behavioral profiles for various entities[11] and then used to find meaningful anomalies. Within
        the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend
        themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow
        researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and
        uncover "hidden insights" through learning from historical relationships and trends in the data.
    </p>
    <p id="application">
        In 2006, the online movie company Netflix held the first "Netflix Prize" competition to find a program to better predict
        user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%.
        A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic
        Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[41] Shortly after the prize was awarded,
        Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation")
        and they changed their recommendation engine accordingly.[42] In 2010 The Wall Street Journal wrote about the firm
        Rebellion Research and their use of Machine Learning to predict the financial crisis. [43] In 2012, co-founder of
        Sun Microsystems Vinod Khosla predicted that 80% of medical doctors jobs would be lost in the next two decades to
        automated machine learning medical diagnostic software.[44] In 2014, it has been reported that a machine learning
        algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized
        influences between artists.

    </p>

    <p id="ethics">
        Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit
        these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[50] For example, using job hiring
        data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring
        job applicants against similarity to previous successful applicants.[51][52] Responsible collection of data and documentation
        of algorithmic rules used by a system thus is a critical part of machine learning. Because language contains biases,
        machines trained on language corpora will necessarily also learn bias.[53] Other forms of ethical challenges, not
        related to personal biases, are more seen in health care. There are concerns among health care professionals that
        these systems might not be designed in the public's interest, but as income generating machines. This is especially
        true in the United States where there is a perpetual ethical dilemma of improving health care, but also increasing
        profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in
        which the algorithm's proprietary owners hold stakes in. There is huge potential for machine learning in health care
        to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this
        will not happen until the personal biases mentioned previously, and these "greed" biases are addressed.
    </p>
    <p id="approaches">
        Decision tree learning Main article: Decision tree learning Decision tree learning uses a decision tree as a predictive model,
        which maps observations about an item to conclusions about the item's target value. Association rule learning Main
        article: Association rule learning Association rule learning is a method for discovering interesting relations between
        variables in large databases. Artificial neural networks Main article: Artificial neural network An artificial neural
        network (ANN) learning algorithm, usually called "neural network" (NN), is a learning algorithm that is vaguely inspired
        by biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons,
        processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical
        data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns
        in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.
        Deep learning Main article: Deep learning Falling hardware prices and the development of GPUs for personal use in
        the last few years have contributed to the development of the concept of deep learning which consists of multiple
        hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light
        and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[26]
        Inductive logic programming Main article: Inductive logic programming Inductive logic programming (ILP) is an approach
        to rule learning using logic programming as a uniform representation for input examples, background knowledge, and
        hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database
        of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples.
        Inductive programming is a related field that considers any kind of programming languages for representing hypotheses
        (and not only logic programming), such as functional programs. Support vector machines Main article: Support vector
        machines Support vector machines (SVMs) are a set of related supervised learning methods used for classification
        and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training
        algorithm builds a model that predicts whether a new example falls into one category or the other. Clustering Main
        article: Cluster analysis Cluster analysis is the assignment of a set of observations into subsets (called clusters)
        so that observations within the same cluster are similar according to some predesignated criterion or criteria, while
        observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions
        on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness
        (similarity between members of the same cluster) and separation between different clusters. Other methods are based
        on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique
        for statistical data analysis. Bayesian networks Main article: Bayesian network A Bayesian network, belief network
        or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables
        and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent
        the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute
        the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.
        Reinforcement learning Main article: Reinforcement learning Reinforcement learning is concerned with how an agent
        ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning
        algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those
        states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are
        never presented, nor sub-optimal actions explicitly corrected. Representation learning Main article: Representation
        learning Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations
        of the inputs provided during training. Classical examples include principal components analysis and cluster analysis.
        Representation learning algorithms often attempt to preserve the information in their input but transform it in a
        way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing
        reconstruction of the inputs coming from the unknown data generating distribution, while not being necessarily faithful
        for configurations that are implausible under that distribution. Manifold learning algorithms attempt to do so under
        the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under
        the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms
        aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without
        reshaping them into (high-dimensional) vectors.[27] Deep learning algorithms discover multiple levels of representation,
        or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level
        features. It has been argued that an intelligent machine is one that learns a representation that disentangles the
        underlying factors of variation that explain the observed data.[28] Similarity and metric learning Main article:
        Similarity learning In this problem, the learning machine is given pairs of examples that are considered similar
        and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that
        can predict if new objects are similar. It is sometimes used in Recommendation systems. Sparse dictionary learning
        Main article: Sparse dictionary learning In this method, a datum is represented as a linear combination of basis
        functions, and the coefficients are assumed to be sparse. Let x be a d-dimensional datum, D be a d by n matrix, where
        each column of D represents a basis function. r is the coefficient to represent x using D. Mathematically, sparse
        dictionary learning means solving {\displaystyle x\approx Dr} {\displaystyle x\approx Dr} where r is sparse. Generally
        speaking, n is assumed to be larger than d to allow the freedom for a sparse representation. Learning a dictionary
        along with sparse representations is strongly NP-hard and also difficult to solve approximately.[29] A popular heuristic
        method for sparse dictionary learning is K-SVD. Sparse dictionary learning has been applied in several contexts.
        In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary
        for each class has already been built. Then a new datum is associated with the class such that it's best sparsely
        represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising.
        The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[30]
        Genetic algorithms Main article: Genetic algorithm A genetic algorithm (GA) is a search heuristic that mimics the
        process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope
        of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s
        and 1990s.[31][32] Conversely, machine learning techniques have been used to improve the performance of genetic and
        evolutionary algorithms.[33] Rule-based machine learning Rule-based machine learning is a general term for any machine
        learning method that identifies, learns, or evolves "rules" to store, manipulate or apply, knowledge. The defining
        characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules
        that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that
        commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[34]
        Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial
        immune systems. Learning classifier systems Main article: Learning classifier system Learning classifier systems
        (LCS) are a family of rule-based machine learning algorithms that combine a discovery component (e.g. typically a
        genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised
        learning). They seek to identify a set of context-dependent rules that collectively store and apply knowledge in
        a piecewise manner in order to make predictions
    </p>
    <script>

            // document.addEventListener("click",change);
             function changeFunction(){
                 alert("current"+ event.newURL);
             }    
         </script>
</body>

</html>